package tokeniser

import (
	_ "embed"
	"errors"
	"reflect"
	"testing"

	"github.com/wtsi-hgi/uber-recipe-creator/internal/testdata"
)

func TestTokeniser(t *testing.T) {
	for n, test := range [...]struct {
		Input  string
		Tokens []Token
		Err    error
	}{
		{ // 1
			"",
			nil,
			nil,
		},
		{ // 2
			"a",
			[]Token{{"a", TokenIdentifier}},
			nil,
		},
		{ // 3
			"abc",
			[]Token{{"abc", TokenIdentifier}},
			nil,
		},
		{ // 4
			"123",
			[]Token{{"123", TokenNumber}},
			nil,
		},
		{ // 5
			"123.456",
			[]Token{{"123.456", TokenNumber}},
			nil,
		},
		{ // 6
			"0.1_2_3",
			[]Token{{"0.1_2_3", TokenNumber}},
			nil,
		},
		{ // 7
			"1_2_3e4",
			[]Token{{"1_2_3e4", TokenNumber}},
			nil,
		},
		{ // 8
			"1e_3",
			[]Token{{"1e_3", TokenNumber}},
			nil,
		},
		{ // 9
			"2j",
			[]Token{{"2j", TokenNumber}},
			nil,
		},
		{ // 10
			"077e010",
			[]Token{{"077e010", TokenNumber}},
			nil,
		},
		{ // 11
			"07_7e010",
			[]Token{{"07_7e010", TokenNumber}},
			nil,
		},
		{ // 12
			"0e1",
			[]Token{{"0e1", TokenNumber}},
			nil,
		},
		{ // 13
			"\"hello, world\"",
			[]Token{{"\"hello, world\"", TokenString}},
			nil,
		},
		{ // 14
			"stats.parse",
			[]Token{
				{"stats", TokenIdentifier},
				{".", TokenDelimiter},
				{"parse", TokenIdentifier},
			},
			nil,
		},
		{ // 15
			"a = 1\nb = \"abc\"",
			[]Token{
				{"a", TokenIdentifier},
				{" ", TokenWhitespace},
				{"=", TokenDelimiter},
				{" ", TokenWhitespace},
				{"1", TokenNumber},
				{"\n", TokenNewline},
				{"b", TokenIdentifier},
				{" ", TokenWhitespace},
				{"=", TokenDelimiter},
				{" ", TokenWhitespace},
				{"\"abc\"", TokenString},
			},
			nil,
		},
		{ // 16
			"if a in b:\n\tasync c(a)",
			[]Token{
				{"if", TokenKeyword},
				{" ", TokenWhitespace},
				{"a", TokenIdentifier},
				{" ", TokenWhitespace},
				{"in", TokenKeyword},
				{" ", TokenWhitespace},
				{"b", TokenIdentifier},
				{":", TokenDelimiter},
				{"\n", TokenNewline},
				{"\t", TokenWhitespace},
				{"async", TokenKeyword},
				{" ", TokenWhitespace},
				{"c", TokenIdentifier},
				{"(", TokenDelimiter},
				{"a", TokenIdentifier},
				{")", TokenDelimiter},
			},
			nil,
		},
		{ // 17
			`"a 'string"`,
			[]Token{{`"a 'string"`, TokenString}},
			nil,
		},
		{ // 18
			`a string"`,
			nil,
			errors.New("eof"),
		},
		{ // 19
			`"a string`,
			nil,
			errors.New("eof"),
		},
		{ // 20
			`r"a string"`,
			[]Token{{`r"a string"`, TokenString}},
			nil,
		},
		{ // 21
			`f"a string"`,
			[]Token{{`f"a string"`, TokenString}},
			nil,
		},
		{ // 22
			`U"a string"`,
			[]Token{{`U"a string"`, TokenString}},
			nil,
		},
		{ // 23
			`rF"a string"`,
			[]Token{{`rF"a string"`, TokenString}},
			nil,
		},
		{ // 24
			`fR"a string"`,
			[]Token{{`fR"a string"`, TokenString}},
			nil,
		},
		{ // 25
			`b"a string"`,
			[]Token{{`b"a string"`, TokenString}},
			nil,
		},
		{ // 26
			`BR"a string"`,
			[]Token{{`BR"a string"`, TokenString}},
			nil,
		},
		{ // 27
			`'a string'`,
			[]Token{{`'a string'`, TokenString}},
			nil,
		},
		{ // 28
			`BR'a string'`,
			[]Token{{`BR'a string'`, TokenString}},
			nil,
		},
		{ // 29
			`BR"""a string"""`,
			[]Token{{`BR"""a string"""`, TokenString}},
			nil,
		},
		{ // 30
			"BR\"\"\"a string\n\n\nhello\"\"\"",
			[]Token{{"BR\"\"\"a string\n\n\nhello\"\"\"", TokenString}},
			nil,
		},
		{ // 31
			`BR'''a string'''`,
			[]Token{{`BR'''a string'''`, TokenString}},
			nil,
		},
		{ // 32
			`"""a string"""`,
			[]Token{{`"""a string"""`, TokenString}},
			nil,
		},
		{ // 33
			`'''a string'''`,
			[]Token{{`'''a string'''`, TokenString}},
			nil,
		},
		{ // 34
			`# a comment!!!!`,
			[]Token{{`# a comment!!!!`, TokenComment}},
			nil,
		},
		{ // 35
			"a = 2 # a comment!!!!\nb",
			[]Token{
				{`a`, TokenIdentifier},
				{` `, TokenWhitespace},
				{`=`, TokenDelimiter},
				{` `, TokenWhitespace},
				{`2`, TokenNumber},
				{` `, TokenWhitespace},
				{`# a comment!!!!`, TokenComment},
				{"\n", TokenNewline},
				{`b`, TokenIdentifier},
			},
			nil,
		},
		{ // 36
			"",
			nil,
			nil,
		},
		{ // 37
			" ",
			[]Token{
				{Type: TokenWhitespace, Val: " "},
			},
			nil,
		},
		{ // 38
			" \t",
			[]Token{
				{Type: TokenWhitespace, Val: " \t"},
			},
			nil,
		},
		{ // 39
			"{ \n }",
			[]Token{
				{Type: TokenDelimiter, Val: "{"},
				{Type: TokenWhitespace, Val: " \n "},
				{Type: TokenDelimiter, Val: "}"},
			},
			nil,
		},
		{ // 40
			"\"\"",
			[]Token{
				{Type: TokenString, Val: "\"\""},
			},
			nil,
		},
		{ // 41
			"\"\\\"\"",
			[]Token{
				{Type: TokenString, Val: "\"\\\"\""},
			},
			nil,
		},
		{ // 42
			"\"\\n\"",
			[]Token{
				{Type: TokenString, Val: "\"\\n\""},
			},
			nil,
		},
		{ // 43
			"\"\\0\"",
			[]Token{
				{Type: TokenString, Val: "\"\\0\""},
			},
			nil,
		},
		{ // 44
			"\"\\x20\"",
			[]Token{
				{Type: TokenString, Val: "\"\\x20\""},
			},
			nil,
		},
		{ // 45
			"\"\\u2020\"",
			[]Token{
				{Type: TokenString, Val: "\"\\u2020\""},
			},
			nil,
		},
		{ // 46
			"b\"abc123\\\"\\'\"",
			[]Token{
				{Type: TokenString, Val: "b\"abc123\\\"\\'\""},
			},
			nil,
		},
		{ // 47
			"B\"abc123\\\"\\'\"",
			[]Token{
				{Type: TokenString, Val: "B\"abc123\\\"\\'\""},
			},
			nil,
		},
		{ // 48
			"f\"abc123\\\"\\'\"",
			[]Token{
				{Type: TokenString, Val: "f\"abc123\\\"\\'\""},
			},
			nil,
		},
		{ // 49
			"F\"abc123\\\"\\'\"",
			[]Token{
				{Type: TokenString, Val: "F\"abc123\\\"\\'\""},
			},
			nil,
		},
		{ // 50
			"\"\"\"abc123\"'456\"\"\"",
			[]Token{
				{Type: TokenString, Val: "\"\"\"abc123\"'456\"\"\""},
			},
			nil,
		},
		{ // 51
			"'''abc123\"'456'''",
			[]Token{
				{Type: TokenString, Val: "'''abc123\"'456'''"},
			},
			nil,
		},
		{ // 52
			"b\"\"\"abc123\"'456\"\"\"",
			[]Token{
				{Type: TokenString, Val: "b\"\"\"abc123\"'456\"\"\""},
			},
			nil,
		},
		{ // 53
			"b'''abc123\"'456'''",
			[]Token{
				{Type: TokenString, Val: "b'''abc123\"'456'''"},
			},
			nil,
		},
		{ // 54
			"f\"\"\"abc123\"'456\"\"\"",
			[]Token{
				{Type: TokenString, Val: "f\"\"\"abc123\"'456\"\"\""},
			},
			nil,
		},
		{ // 55
			"f'''abc123\"'456'''",
			[]Token{
				{Type: TokenString, Val: "f'''abc123\"'456'''"},
			},
			nil,
		},
		{ // 56
			"r\"\"\"abc123\"'456\"\"\"",
			[]Token{
				{Type: TokenString, Val: "r\"\"\"abc123\"'456\"\"\""},
			},
			nil,
		},
		{ // 57
			"r'''abc123\"'456'''",
			[]Token{
				{Type: TokenString, Val: "r'''abc123\"'456'''"},
			},
			nil,
		},
		{ // 58
			"u\"abc123\\\"\\'\"",
			[]Token{
				{Type: TokenString, Val: "u\"abc123\\\"\\'\""},
			},
			nil,
		},
		{ // 59
			"u'abc123\\\"\\''",
			[]Token{
				{Type: TokenString, Val: "u'abc123\\\"\\''"},
			},
			nil,
		},
		{ // 60
			"o\"abc123\\\"'456\"",
			[]Token{
				{Type: TokenIdentifier, Val: "o"},
				{Type: TokenString, Val: "\"abc123\\\"'456\""},
			},
			nil,
		},
		{ // 61
			"o'abc123\"\\'456'",
			[]Token{
				{Type: TokenIdentifier, Val: "o"},
				{Type: TokenString, Val: "'abc123\"\\'456'"},
			},
			nil,
		},
		{ // 62
			"o\"\"\"abc123\"'456\"\"\"",
			[]Token{
				{Type: TokenIdentifier, Val: "o"},
				{Type: TokenString, Val: "\"\"\"abc123\"'456\"\"\""},
			},
			nil,
		},
		{ // 63
			"o'''abc123\"'456'''",
			[]Token{
				{Type: TokenIdentifier, Val: "o"},
				{Type: TokenString, Val: "'''abc123\"'456'''"},
			},
			nil,
		},
		{ // 64
			"\"\"\"abc123\n456\"\"\"",
			[]Token{
				{Type: TokenString, Val: "\"\"\"abc123\n456\"\"\""},
			},
			nil,
		},
		{ // 65
			"'''abc123\n456'''",
			[]Token{
				{Type: TokenString, Val: "'''abc123\n456'''"},
			},
			nil,
		},
		{ // 66
			"False",
			[]Token{
				{Type: TokenKeyword, Val: "False"},
			},
			nil,
		},
		{ // 67
			"True",
			[]Token{
				{Type: TokenKeyword, Val: "True"},
			},
			nil,
		},
		{ // 68
			"false",
			[]Token{
				{Type: TokenIdentifier, Val: "false"},
			},
			nil,
		},
		{ // 69
			"true",
			[]Token{
				{Type: TokenIdentifier, Val: "true"},
			},
			nil,
		},
		{ // 70
			"None",
			[]Token{
				{Type: TokenKeyword, Val: "None"},
			},
			nil,
		},
		{ // 71
			"none",
			[]Token{
				{Type: TokenIdentifier, Val: "none"},
			},
			nil,
		},
		{ // 72
			"# A Comment\n\"A string\"\n\"another string\"# Another Comment\n\"\"",
			[]Token{
				{Type: TokenComment, Val: "# A Comment"},
				{Type: TokenNewline, Val: "\n"},
				{Type: TokenString, Val: "\"A string\""},
				{Type: TokenNewline, Val: "\n"},
				{Type: TokenString, Val: "\"another string\""},
				{Type: TokenComment, Val: "# Another Comment"},
				{Type: TokenNewline, Val: "\n"},
				{Type: TokenString, Val: "\"\""},
			},
			nil,
		},
		{ // 73
			"identifier",
			[]Token{
				{Type: TokenIdentifier, Val: "identifier"},
			},
			nil,
		},
		{ // 74
			"another identifier",
			[]Token{
				{Type: TokenIdentifier, Val: "another"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "identifier"},
			},
			nil,
		},
		{ // 75
			"f r u fR rB farm",
			[]Token{
				{Type: TokenIdentifier, Val: "f"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "r"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "u"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "fR"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "rB"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenIdentifier, Val: "farm"},
			},
			nil,
		},
		{ // 76
			"await if for else global not yield from",
			[]Token{
				{Type: TokenKeyword, Val: "await"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "if"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "for"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "else"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "global"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "not"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "yield"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenKeyword, Val: "from"},
			},
			nil,
		},
		{ // 77
			"+ % | = == : := - * ** < >= != ~",
			[]Token{
				{Type: TokenOperator, Val: "+"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "%"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "|"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "=="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ":"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: ":="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "-"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "*"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "**"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "<"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: ">="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "!="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenOperator, Val: "~"},
			},
			nil,
		},
		{ // 78
			"+= %= |= -= -> *= **= /= //= <<= >>= , . ;",
			[]Token{
				{Type: TokenDelimiter, Val: "+="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "%="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "|="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "-="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "->"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "*="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "**="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "/="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "//="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "<<="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ">>="},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ","},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "."},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ";"},
			},
			nil,
		},
		{ // 79
			"( )",
			[]Token{
				{Type: TokenDelimiter, Val: "("},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ")"},
			},
			nil,
		},
		{ // 80
			"[ ]",
			[]Token{
				{Type: TokenDelimiter, Val: "["},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "]"},
			},
			nil,
		},
		{ // 81
			"{ ( [ ] ) }",
			[]Token{
				{Type: TokenDelimiter, Val: "{"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "("},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "["},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "]"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: ")"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenDelimiter, Val: "}"},
			},
			nil,
		},
		{ // 82
			"7 2147483647 0o177 0b100110111 3 79228162514264337593543950336 0o377 0xdeadbeef 100_000_000_000 0b_1110_0101",
			[]Token{
				{Type: TokenNumber, Val: "7"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "2147483647"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0o177"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0b100110111"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "3"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "79228162514264337593543950336"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0o377"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0xdeadbeef"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "100_000_000_000"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0b_1110_0101"},
			},
			nil,
		},
		{ // 83
			"3.14 10. .001 1e100 3.14e-10 0e0 3.14_15_93",
			[]Token{
				{Type: TokenNumber, Val: "3.14"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "10."},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: ".001"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "1e100"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "3.14e-10"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "0e0"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "3.14_15_93"},
			},
			nil,
		},
		{ // 84
			"3.14j 10.j 10j .001j 1e100j 3.14e-10j 3.14_15_93j",
			[]Token{
				{Type: TokenNumber, Val: "3.14j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "10.j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "10j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: ".001j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "1e100j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "3.14e-10j"},
				{Type: TokenWhitespace, Val: " "},
				{Type: TokenNumber, Val: "3.14_15_93j"},
			},
			nil,
		},
		{ // 85
			"a(1,\n2, \n3\n )\nb()",
			[]Token{
				{Type: TokenIdentifier, Val: "a"},
				{Type: TokenDelimiter, Val: "("},
				{Type: TokenNumber, Val: "1"},
				{Type: TokenDelimiter, Val: ","},
				{Type: TokenWhitespace, Val: "\n"},
				{Type: TokenNumber, Val: "2"},
				{Type: TokenDelimiter, Val: ","},
				{Type: TokenWhitespace, Val: " \n"},
				{Type: TokenNumber, Val: "3"},
				{Type: TokenWhitespace, Val: "\n "},
				{Type: TokenDelimiter, Val: ")"},
				{Type: TokenNewline, Val: "\n"},
				{Type: TokenIdentifier, Val: "b"},
				{Type: TokenDelimiter, Val: "("},
				{Type: TokenDelimiter, Val: ")"},
			},
			nil,
		},
		{ // 86
			"a[1,\n2, \n3\n ]\n",
			[]Token{
				{Type: TokenIdentifier, Val: "a"},
				{Type: TokenDelimiter, Val: "["},
				{Type: TokenNumber, Val: "1"},
				{Type: TokenDelimiter, Val: ","},
				{Type: TokenWhitespace, Val: "\n"},
				{Type: TokenNumber, Val: "2"},
				{Type: TokenDelimiter, Val: ","},
				{Type: TokenWhitespace, Val: " \n"},
				{Type: TokenNumber, Val: "3"},
				{Type: TokenWhitespace, Val: "\n "},
				{Type: TokenDelimiter, Val: "]"},
				{Type: TokenNewline, Val: "\n"},
			},
			nil,
		},
		{ // 87
			"a\\\nb",
			[]Token{
				{Type: TokenIdentifier, Val: "a"},
				{Type: TokenWhitespace, Val: "\\\n"},
				{Type: TokenIdentifier, Val: "b"},
			},
			nil,
		},
	} {
		tokens, err := Tokenise(test.Input)
		if !reflect.DeepEqual(err, test.Err) {
			t.Errorf("Test %d: input was %s, expecting error %s, got %s", n+1, test.Input, test.Err, err)
		} else if !reflect.DeepEqual(tokens, test.Tokens) {
			t.Errorf("Test %d: got %v, want %v", n+1, tokens, test.Tokens)
		}
	}
}

func TestPythonFile(t *testing.T) {
	expected := []Token{
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"hashlib", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"pyreadr", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"tempfile", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"pandas", TokenIdentifier},
		{" ", TokenWhitespace},
		{"as", TokenKeyword},
		{" ", TokenWhitespace},
		{"pd", TokenIdentifier},
		{"\n\n", TokenNewline},
		{"databaseurl", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://cran.r-project.org/web/packages/packages.rds\"", TokenString},
		{"\n\n", TokenNewline},
		{"response", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{".", TokenDelimiter},
		{"get", TokenIdentifier},
		{"(", TokenDelimiter},
		{"databaseurl", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"allow_redirects", TokenIdentifier},
		{"=", TokenDelimiter},
		{"True", TokenKeyword},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"tmpfile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"tempfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"NamedTemporaryFile", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"tmpfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"write", TokenIdentifier},
		{"(", TokenDelimiter},
		{"response", TokenIdentifier},
		{".", TokenDelimiter},
		{"content", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"database", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pyreadr", TokenIdentifier},
		{".", TokenDelimiter},
		{"read_r", TokenIdentifier},
		{"(", TokenDelimiter},
		{"tmpfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"name", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"database", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"database", TokenIdentifier},
		{"[", TokenDelimiter},
		{"None", TokenKeyword},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"pandasDatabase", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pd", TokenIdentifier},
		{".", TokenDelimiter},
		{"DataFrame", TokenIdentifier},
		{"(", TokenDelimiter},
		{"database", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"urlbool", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"True", TokenKeyword},
		{"\n\n", TokenNewline},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"str", TokenIdentifier},
		{"(", TokenDelimiter},
		{"input", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"Please enter package name: \"", TokenString},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"record", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pandasDatabase", TokenIdentifier},
		{".", TokenDelimiter},
		{"loc", TokenIdentifier},
		{"[", TokenDelimiter},
		{"pandasDatabase", TokenIdentifier},
		{"[", TokenDelimiter},
		{"'Package'", TokenString},
		{"]", TokenDelimiter},
		{" ", TokenWhitespace},
		{"==", TokenOperator},
		{" ", TokenWhitespace},
		{"package", TokenIdentifier},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"name", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"description", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Title\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Description\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{"\n", TokenNewline},
		{"try", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Imports\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"split", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\", \"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"except", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"[", TokenDelimiter},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"try", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"packageURL", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"URL\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{"\n", TokenNewline},
		{"except", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"urlbool", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"False", TokenKeyword},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"\\\"\\\"\\\"\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"name", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"description", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"\\\"\\\"\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"if", TokenKeyword},
		{" ", TokenWhitespace},
		{"urlbool", TokenIdentifier},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"homepage = \\\"{packageURL}\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"cran = \\\"{package}\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"source", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{".", TokenDelimiter},
		{"get", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"https://cran.r-project.org/src/contrib/\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\"_\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Version\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\".tar.gz\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"allow_redirects", TokenIdentifier},
		{"=", TokenDelimiter},
		{"True", TokenKeyword},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"sha256_hash", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"hashlib", TokenIdentifier},
		{".", TokenDelimiter},
		{"sha256", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"sha256_hash", TokenIdentifier},
		{".", TokenDelimiter},
		{"update", TokenIdentifier},
		{"(", TokenDelimiter},
		{"source", TokenIdentifier},
		{".", TokenDelimiter},
		{"content", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"version(\\\"{record['Version'].values[0]}\\\", sha256=\\\"{sha256_hash.hexdigest()}\\\")\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"for", TokenKeyword},
		{" ", TokenWhitespace},
		{"k", TokenIdentifier},
		{" ", TokenWhitespace},
		{"in", TokenKeyword},
		{" ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"depends_on(\\\"r-\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"k", TokenIdentifier},
		{".", TokenDelimiter},
		{"lower", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{".", TokenDelimiter},
		{"replace", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\".\"", TokenString},
		{",", TokenDelimiter},
		{"\"-\"", TokenString},
		{")", TokenDelimiter},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\"\\\", type=(\\\"build\\\", \\\"run\\\"))\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
	}

	tokens, _ := Tokenise(testdata.TestScript1)
	if !reflect.DeepEqual(tokens, expected) {
		t.Errorf("failed!")
	}
}

func TestRecipeFile(t *testing.T) {
	expected := []Token{
		{"# Copyright 2013-2023 Lawrence Livermore National Security, LLC and other", TokenComment},
		{"\n", TokenNewline},
		{"# Spack Project Developers. See the top-level COPYRIGHT file for details.", TokenComment},
		{"\n", TokenNewline},
		{"#", TokenComment},
		{"\n", TokenNewline},
		{"# SPDX-License-Identifier: (Apache-2.0 OR MIT)", TokenComment},
		{"\n\n", TokenNewline},
		{"from", TokenKeyword},
		{" ", TokenWhitespace},
		{"spack", TokenIdentifier},
		{".", TokenDelimiter},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"*", TokenOperator},
		{"\n\n\n", TokenNewline},
		{"class", TokenKeyword},
		{" ", TokenWhitespace},
		{"Nextdenovo", TokenIdentifier},
		{"(", TokenDelimiter},
		{"MakefilePackage", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"\"\"\"NextDenovo is a string graph-based de novo assembler for long reads.\n    idk\n    something \n    \n    \n    \n    hello\"\"\"", TokenString},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"homepage", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://nextdenovo.readthedocs.io/en/latest/index.html\"", TokenString},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"url", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://github.com/Nextomics/NextDenovo/archive/refs/tags/2.5.2.tar.gz\"", TokenString},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"version", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"2.5.2\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"sha256", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"f1d07c9c362d850fd737c41e5b5be9d137b1ef3f1aec369dc73c637790611190\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"python\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"py-paralleltask\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"zlib\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"(", TokenDelimiter},
		{"\"build\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"link\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"def", TokenKeyword},
		{" ", TokenWhitespace},
		{"edit", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"spec", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"makefile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"FileFilter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"Makefile\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"makefile", TokenIdentifier},
		{".", TokenDelimiter},
		{"filter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"r\"^TOP_DIR.*\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"TOP_DIR={0}\"", TokenString},
		{".", TokenDelimiter},
		{"format", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{".", TokenDelimiter},
		{"build_directory", TokenIdentifier},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"runfile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"FileFilter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"nextDenovo\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"runfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"filter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"r\"^SCRIPT_PATH.*\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"SCRIPT_PATH = '{0}'\"", TokenString},
		{".", TokenDelimiter},
		{"format", TokenIdentifier},
		{"(", TokenDelimiter},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"def", TokenKeyword},
		{" ", TokenWhitespace},
		{"install", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"spec", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install_tree", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"bin\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"bin", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"nextDenovo\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"bin", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install_tree", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"lib\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"lib", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
	}

	tokens, _ := Tokenise(testdata.TestRecipe1)
	if !reflect.DeepEqual(tokens, expected) {
		t.Errorf("failed!")
	}
}
