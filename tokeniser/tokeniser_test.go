package tokeniser

import (
	_ "embed"
	"errors"
	"reflect"
	"testing"
)

func TestTokeniser(t *testing.T) {
	for n, test := range [...]struct {
		Input  string
		Tokens []Token
		Err    error
	}{
		{"", nil, nil},
		{"a", []Token{{"a", TokenIdentifier}}, nil},
		{"abc", []Token{{"abc", TokenIdentifier}}, nil},
		{"123", []Token{{"123", TokenNumber}}, nil},
		{"123.456", []Token{{"123.456", TokenNumber}}, nil},
		{"0.1_2_3", []Token{{"0.1_2_3", TokenNumber}}, nil},
		{"1_2_3e4", []Token{{"1_2_3e4", TokenNumber}}, nil},
		{"1e_3", []Token{{"1e_3", TokenNumber}}, nil},
		{"2j", []Token{{"2j", TokenNumber}}, nil},
		{"077e010", []Token{{"077e010", TokenNumber}}, nil},
		{"07_7e010", []Token{{"07_7e010", TokenNumber}}, nil},
		{"0e1", []Token{{"0e1", TokenNumber}}, nil},
		{"\"hello, world\"", []Token{{"\"hello, world\"", TokenString}}, nil},
		{"stats.parse", []Token{
			{"stats", TokenIdentifier},
			{".", TokenDelimiter},
			{"parse", TokenIdentifier},
		}, nil},
		{"a = 1\nb = \"abc\"", []Token{
			{"a", TokenIdentifier},
			{" ", TokenWhitespace},
			{"=", TokenDelimiter},
			{" ", TokenWhitespace},
			{"1", TokenNumber},
			{"\n", TokenNewline},
			{"b", TokenIdentifier},
			{" ", TokenWhitespace},
			{"=", TokenDelimiter},
			{" ", TokenWhitespace},
			{"\"abc\"", TokenString},
		}, nil},
		{"if a in b:\n\tasync c(a)", []Token{
			{"if", TokenKeyword},
			{" ", TokenWhitespace},
			{"a", TokenIdentifier},
			{" ", TokenWhitespace},
			{"in", TokenKeyword},
			{" ", TokenWhitespace},
			{"b", TokenIdentifier},
			{":", TokenDelimiter},
			{"\n", TokenNewline},
			{"\t", TokenWhitespace},
			{"async", TokenKeyword},
			{" ", TokenWhitespace},
			{"c", TokenIdentifier},
			{"(", TokenDelimiter},
			{"a", TokenIdentifier},
			{")", TokenDelimiter},
		}, nil},
		{`"a 'string"`, []Token{{`"a 'string"`, TokenString}}, nil},
		{`a string"`, nil, errors.New("eof")},
		{`"a string`, nil, errors.New("eof")},
		{`r"a string"`, []Token{{`r"a string"`, TokenString}}, nil},
		{`f"a string"`, []Token{{`f"a string"`, TokenString}}, nil},
		{`U"a string"`, []Token{{`U"a string"`, TokenString}}, nil},
		{`rF"a string"`, []Token{{`rF"a string"`, TokenString}}, nil},
		{`fR"a string"`, []Token{{`fR"a string"`, TokenString}}, nil},
		{`b"a string"`, []Token{{`b"a string"`, TokenString}}, nil},
		{`BR"a string"`, []Token{{`BR"a string"`, TokenString}}, nil},
		{`'a string'`, []Token{{`'a string'`, TokenString}}, nil},
		{`BR'a string'`, []Token{{`BR'a string'`, TokenString}}, nil},
		{`BR"""a string"""`, []Token{{`BR"""a string"""`, TokenString}}, nil},
		{"BR\"\"\"a string\n\n\nhello\"\"\"", []Token{{"BR\"\"\"a string\n\n\nhello\"\"\"", TokenString}}, nil},
		{`BR'''a string'''`, []Token{{`BR'''a string'''`, TokenString}}, nil},
		{`"""a string"""`, []Token{{`"""a string"""`, TokenString}}, nil},
		{`'''a string'''`, []Token{{`'''a string'''`, TokenString}}, nil},
		{`# a comment!!!!`, []Token{{`# a comment!!!!`, TokenComment}}, nil},
		{"a = 2 # a comment!!!!\nb", []Token{
			{`a`, TokenIdentifier},
			{` `, TokenWhitespace},
			{`=`, TokenDelimiter},
			{` `, TokenWhitespace},
			{`2`, TokenNumber},
			{` `, TokenWhitespace},
			{`# a comment!!!!`, TokenComment},
			{"\n", TokenNewline},
			{`b`, TokenIdentifier},
		}, nil},
	} {
		tokens, err := Tokenise(test.Input)
		if !reflect.DeepEqual(err, test.Err) {
			t.Errorf("Test %d: input was %s, expecting error %s, got %s", n+1, test.Input, test.Err, err)
		} else if !reflect.DeepEqual(tokens, test.Tokens) {
			t.Errorf("Test %d: got %v, want %v", n+1, tokens, test.Tokens)
		}
	}
}

//go:embed test.py
var testFile string

//go:embed package.py
var testRecipe string

func TestPythonFile(t *testing.T) {
	expected := []Token{{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"hashlib", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"pyreadr", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"tempfile", TokenIdentifier},
		{"\n", TokenNewline},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"pandas", TokenIdentifier},
		{" ", TokenWhitespace},
		{"as", TokenKeyword},
		{" ", TokenWhitespace},
		{"pd", TokenIdentifier},
		{"\n\n", TokenNewline},
		{"databaseurl", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://cran.r-project.org/web/packages/packages.rds\"", TokenString},
		{"\n\n", TokenNewline},
		{"response", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{".", TokenDelimiter},
		{"get", TokenIdentifier},
		{"(", TokenDelimiter},
		{"databaseurl", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"allow_redirects", TokenIdentifier},
		{"=", TokenDelimiter},
		{"True", TokenKeyword},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"tmpfile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"tempfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"NamedTemporaryFile", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"tmpfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"write", TokenIdentifier},
		{"(", TokenDelimiter},
		{"response", TokenIdentifier},
		{".", TokenDelimiter},
		{"content", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"database", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pyreadr", TokenIdentifier},
		{".", TokenDelimiter},
		{"read_r", TokenIdentifier},
		{"(", TokenDelimiter},
		{"tmpfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"name", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"database", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"database", TokenIdentifier},
		{"[", TokenDelimiter},
		{"None", TokenKeyword},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"pandasDatabase", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pd", TokenIdentifier},
		{".", TokenDelimiter},
		{"DataFrame", TokenIdentifier},
		{"(", TokenDelimiter},
		{"database", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"urlbool", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"True", TokenKeyword},
		{"\n\n", TokenNewline},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"str", TokenIdentifier},
		{"(", TokenDelimiter},
		{"input", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"Please enter package name: \"", TokenString},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"record", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"pandasDatabase", TokenIdentifier},
		{".", TokenDelimiter},
		{"loc", TokenIdentifier},
		{"[", TokenDelimiter},
		{"pandasDatabase", TokenIdentifier},
		{"[", TokenDelimiter},
		{"'Package'", TokenString},
		{"]", TokenDelimiter},
		{" ", TokenWhitespace},
		{"==", TokenOperator},
		{" ", TokenWhitespace},
		{"package", TokenIdentifier},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"name", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"description", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Title\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Description\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{"\n", TokenNewline},
		{"try", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Imports\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"split", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\", \"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"except", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"[", TokenDelimiter},
		{"]", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"try", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"packageURL", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"URL\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{"\n", TokenNewline},
		{"except", TokenKeyword},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"urlbool", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"False", TokenKeyword},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"\\\"\\\"\\\"\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"name", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"description", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"\\\"\\\"\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"if", TokenKeyword},
		{" ", TokenWhitespace},
		{"urlbool", TokenIdentifier},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"homepage = \\\"{packageURL}\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"cran = \\\"{package}\\\"\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"source", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"requests", TokenIdentifier},
		{".", TokenDelimiter},
		{"get", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"https://cran.r-project.org/src/contrib/\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\"_\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"record", TokenIdentifier},
		{"[", TokenDelimiter},
		{"\"Version\"", TokenString},
		{"]", TokenDelimiter},
		{".", TokenDelimiter},
		{"values", TokenIdentifier},
		{"[", TokenDelimiter},
		{"0", TokenNumber},
		{"]", TokenDelimiter},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\".tar.gz\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"allow_redirects", TokenIdentifier},
		{"=", TokenDelimiter},
		{"True", TokenKeyword},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"sha256_hash", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"hashlib", TokenIdentifier},
		{".", TokenDelimiter},
		{"sha256", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"sha256_hash", TokenIdentifier},
		{".", TokenDelimiter},
		{"update", TokenIdentifier},
		{"(", TokenDelimiter},
		{"source", TokenIdentifier},
		{".", TokenDelimiter},
		{"content", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"f\"version(\\\"{record['Version'].values[0]}\\\", sha256=\\\"{sha256_hash.hexdigest()}\\\")\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"for", TokenKeyword},
		{" ", TokenWhitespace},
		{"k", TokenIdentifier},
		{" ", TokenWhitespace},
		{"in", TokenKeyword},
		{" ", TokenWhitespace},
		{"dependencies", TokenIdentifier},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"depends_on(\\\"r-\"", TokenString},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"k", TokenIdentifier},
		{".", TokenDelimiter},
		{"lower", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{".", TokenDelimiter},
		{"replace", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\".\"", TokenString},
		{",", TokenDelimiter},
		{"\"-\"", TokenString},
		{")", TokenDelimiter},
		{" ", TokenWhitespace},
		{"+", TokenOperator},
		{" ", TokenWhitespace},
		{"\"\\\", type=(\\\"build\\\", \\\"run\\\"))\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"print", TokenIdentifier},
		{"(", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
	}

	tokens, _ := Tokenise(testFile)
	if !reflect.DeepEqual(tokens, expected) {
		t.Errorf("failed!")
	}
}

func TestRecipeFile(t *testing.T) {
	expected := []Token{{"# Copyright 2013-2023 Lawrence Livermore National Security, LLC and other", TokenComment},
		{"\n", TokenNewline},
		{"# Spack Project Developers. See the top-level COPYRIGHT file for details.", TokenComment},
		{"\n", TokenNewline},
		{"#", TokenComment},
		{"\n", TokenNewline},
		{"# SPDX-License-Identifier: (Apache-2.0 OR MIT)", TokenComment},
		{"\n\n", TokenNewline},
		{"from", TokenKeyword},
		{" ", TokenWhitespace},
		{"spack", TokenIdentifier},
		{".", TokenDelimiter},
		{"package", TokenIdentifier},
		{" ", TokenWhitespace},
		{"import", TokenKeyword},
		{" ", TokenWhitespace},
		{"*", TokenOperator},
		{"\n\n\n", TokenNewline},
		{"class", TokenKeyword},
		{" ", TokenWhitespace},
		{"Nextdenovo", TokenIdentifier},
		{"(", TokenDelimiter},
		{"MakefilePackage", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"\"\"\"NextDenovo is a string graph-based de novo assembler for long reads.\n    idk\n    something \n    \n    \n    \n    hello\"\"\"", TokenString},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"homepage", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://nextdenovo.readthedocs.io/en/latest/index.html\"", TokenString},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"url", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"https://github.com/Nextomics/NextDenovo/archive/refs/tags/2.5.2.tar.gz\"", TokenString},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"version", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"2.5.2\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"sha256", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"f1d07c9c362d850fd737c41e5b5be9d137b1ef3f1aec369dc73c637790611190\"", TokenString},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"python\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"py-paralleltask\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"depends_on", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"zlib\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"type", TokenIdentifier},
		{"=", TokenDelimiter},
		{"(", TokenDelimiter},
		{"\"build\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"link\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"run\"", TokenString},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"def", TokenKeyword},
		{" ", TokenWhitespace},
		{"edit", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"spec", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"makefile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"FileFilter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"Makefile\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"makefile", TokenIdentifier},
		{".", TokenDelimiter},
		{"filter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"r\"^TOP_DIR.*\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"TOP_DIR={0}\"", TokenString},
		{".", TokenDelimiter},
		{"format", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{".", TokenDelimiter},
		{"build_directory", TokenIdentifier},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"runfile", TokenIdentifier},
		{" ", TokenWhitespace},
		{"=", TokenDelimiter},
		{" ", TokenWhitespace},
		{"FileFilter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"nextDenovo\"", TokenString},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"runfile", TokenIdentifier},
		{".", TokenDelimiter},
		{"filter", TokenIdentifier},
		{"(", TokenDelimiter},
		{"r\"^SCRIPT_PATH.*\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"\"SCRIPT_PATH = '{0}'\"", TokenString},
		{".", TokenDelimiter},
		{"format", TokenIdentifier},
		{"(", TokenDelimiter},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{")", TokenDelimiter},
		{"\n\n", TokenNewline},
		{"    ", TokenWhitespace},
		{"def", TokenKeyword},
		{" ", TokenWhitespace},
		{"install", TokenIdentifier},
		{"(", TokenDelimiter},
		{"self", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"spec", TokenIdentifier},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{")", TokenDelimiter},
		{":", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install_tree", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"bin\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"bin", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"nextDenovo\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"bin", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
		{"        ", TokenWhitespace},
		{"install_tree", TokenIdentifier},
		{"(", TokenDelimiter},
		{"\"lib\"", TokenString},
		{",", TokenDelimiter},
		{" ", TokenWhitespace},
		{"prefix", TokenIdentifier},
		{".", TokenDelimiter},
		{"lib", TokenIdentifier},
		{")", TokenDelimiter},
		{"\n", TokenNewline},
	}

	tokens, _ := Tokenise(testRecipe)
	if !reflect.DeepEqual(tokens, expected) {
		t.Errorf("failed!")
	}
}
